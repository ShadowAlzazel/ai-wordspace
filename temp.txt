What is a Token Embeddings? Token embeddings are a foundational concept in large language models (LLMs), serving as the bridge between raw text and the high-dimensional numerical computations that neural networks perform. Since characters are a uman social construct, it is hard to map what the semantics actually mean. During the outset of processing text, each token—whether a word, subword, or character—is mapped to a dense vector in a high-dimensional space. 
These vectors, called embeddings, are learned during training and are designed to capture semantic and syntactic relationships between tokens. For instance, in a well-trained model, the vectors for semantically similar words (like “cat” and “dog”) will be close together in this vector space, even though their original token IDs might be very different.
As an input token flows through the layers of an LLM, its representation transforms repeatedly. Initially, the token embedding provides a static location in the embedding space, but as it passes through attention layers, feed-forward networks, and non-linearities, it begins to encode contextual information. That is, while the initial embedding might tell you what the token "is," the deeper layer activations describe what the token "means" in its particular context. 
These internal vector spaces—often called hidden states (or “Wordspaces”)—are shaped by the weights and architecture of the model, and they evolve across layers in a structured way. In some sense, each layer of the network builds a different view of the token space, with deeper layers capturing more abstract relationships.
Interestingly, despite the complexity of these transformations, the geometry of the embedding and hidden state spaces often exhibits surprising structure. Researchers have found that the distribution of these vectors tends to concentrate in certain regions of the space, often aligning with directions or planes that encode specific linguistic features. This brings us naturally to the concept of Normal Distributions: many initialization schemes and theoretical analyses of deep networks rely on assumptions that activations—and sometimes the embeddings themselves—are approximately normally distributed. Understanding how token representations behave under these distributions helps us reason about the stability, expressiveness, and learning dynamics of LLMs.
